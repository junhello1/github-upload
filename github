import re
import requests
import threading
import time
import random
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import pymysql


def Change_UA():
    '''
    访问UA切换，引入fake_useragent包
    单次访问单次调用，保证每次UA切换
    共模拟浏览器：ie，chrome，firefox，safari
    :return: UA
    '''
    ua = UserAgent()
    browser = ['ie', 'chrome', 'firefox', 'safari']
    return ua[(browser[random.randint(0, 3)])]


def get_proxy():
    '''
    爬取快代理网站，爬取第一页代理，时间可保证在当天时间，后续页代理不可靠
    其他国内高匿代理网站：https://www.xicidaili.com/nn/
    :return: 代理列表
    '''
    global proxys
    proxys = []
    content = requests.get("https://www.kuaidaili.com/free/inha/1/")
    content.encoding = content.apparent_encoding
    text = content.text
    soup = BeautifulSoup(text, 'html.parser')
    soup = soup.tbody
    IPS = soup.find_all(attrs={'data-title': 'IP'})
    PORTS = soup.find_all(attrs={'data-title': 'PORT'})
    # print('%s:%s'% (IPS[0].string,PORTS[0].string))
    for i in range(0, len(IPS)):
        proxy = '%s:%s' % (IPS[i].string, PORTS[i].string)
        proxys.append(proxy)


def random_proxy():
    '''
    将获取到的proxy进行随机化获取，每次调用该函数，代理不唯一
    :return:   proxy
    '''
    proxy = proxys[random.randint(0, len(proxys) - 1)]
    return proxy



def get(url, referer):
    proxies = {'http': 'http://%s' % (random_proxy())}
    global no_get
    UA = Change_UA()
    headers = {
        'Accept': '*/*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip,deflate,br',
        'Connection': 'Upgrade',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache',
        'User-Agent': UA,
        'referer': referer
    }
    try:
        content = requests.get(url=url, headers=headers, proxies=proxies, timeout=5)
        return content
    except requests.exceptions.RequestException as e:
        print('无法访问：%s' % url)
        return None


def get_referer(url):
    '''
    根据url获取referer
    :param url:
    :return: referer
    '''
    referer = ''
    count = 0
    for i in url:
        if i == '/':
            count = count + 1
        if count == 5:
            break
        referer = referer + i
    return referer


def main():
    count = 0
    get_proxy()
    file = open('D:/github.txt', 'ab+')
    for num in range(5,45):
        # testurl = 'https://github.com/'
        testurl = 'https://github.com/search?p='+str(num)+'&q=%40%2....balabaa...extension%3Aphp&type=Code'
        print(testurl)
        proxies = {'http': 'http://%s' % (random_proxy())}
        print(proxies)
        headers = {
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip,deflate,br',
            'Connection': 'close',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'User-Agent': Change_UA(),
            'cookie':'需要登陆看下自己cookie手动添加',
            'referer': get_referer(url=testurl)
        }
        try:
            resultqqqq = requests.get(url=testurl, headers=headers, proxies=proxies, timeout=100)
        except requests.exceptions.ConnectionError:
            # list.status_code="Connection refused"
            time.sleep(3)
        soup = BeautifulSoup(resultqqqq.text, 'lxml')
        # file = open('D:/github.txt', 'ab+')
        for classlist in soup.find_all(attrs={'class': 'Link--secondary'}):
            print(classlist.text)
            string = classlist.text
            a = '\n'
            file.write(string.encode('utf-8') + a.encode('utf-8'))

    file.close()

if __name__ == '__main__':
    main()
